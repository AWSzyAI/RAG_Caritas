import json
import os
import sys
import requests
session = requests.Session()
import hashlib
import re
import time
import argparse
import datetime
import logging
import threading
import concurrent.futures  

import jieba
import numpy as np
import pandas as pd  # ç”¨äº CSV å¤„ç†
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm  # è¿›åº¦æ¡åº“
from dotenv import load_dotenv
load_dotenv()
from zhipuai import ZhipuAI

from kimi_api import  send_messages
KIMI_API_KEY = os.getenv("KIMI_API_KEY")



# ================================
# EMBEDDING_MODEL = "bge-m3"     #
EMBEDDING_MODEL = "embedding-3"  #
CHUNK_SIZE = 50                  #
# CHUNK_SIZE = 200               #
BATCH_SIZE = 10
SENTENCE_BATCH_SIZE = 100
BATCH_REQUEST_TO_ZHIPU = True #æµ‹è¯•ä¸€æ¬¡æ€§è¯·æ±‚ä¸€ç¯‡æ–‡ç« ä¸­çš„æ‰€æœ‰åˆ‡ç‰‡
MAX_WORKER = 20
# ================================


# ======================================================================
ARTICLES_FILE = "./data/articles.json"
SYSTEM_FILE = "./data/system.json"
log_dir = "logs"
# ======================================================================
if EMBEDDING_MODEL == "bge-m3":
    # bge-m3
    API_URL = os.getenv("API_URL_siliconflow")
    API_KEY = os.getenv("API_KEY_siliconflow")
    MODEL_NAME = "BAAI/bge-m3"
    # zhipu embedding-3
elif EMBEDDING_MODEL == "embedding-3":
    MODEL_NAME = "embedding-3"
    API_URL = os.getenv("API_URL_ZHIPU")
    API_KEY = os.getenv("ZHIPU_API_key")
    zhipu_client = ZhipuAI(api_key=API_KEY)

# ========================================================================
os.makedirs(log_dir, exist_ok=True)
log_filename = os.path.join(log_dir, f"log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
cache_dir = os.path.join(".", "cache", f"{EMBEDDING_MODEL}-{CHUNK_SIZE}")
OUTPUT_DIR = os.path.join(".", "output")
os.makedirs(cache_dir, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
METADATA_FILE = f"{cache_dir}/metadata_cache.json"
EMBEDDING_FILE = f"{cache_dir}/embedding_vectors.json"
SYSTEM_METADATA_FILE = f"{cache_dir}/system_metadata_cache.json"
SYSTEM_EMBEDDING_FILE = f"{cache_dir}/system_embedding_vectors.json"
INDEX_FILE = f"{cache_dir}/index.csv"
FAIL_FILE = f"{cache_dir}/fail.csv"

UNPROCESSED = "âŒ"
PROCESSED = "âœ…"
# =========================================================================
# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    filename=log_filename,  # æŠŠæ—¥å¿—å†™å…¥æ–‡ä»¶
    filemode='a'  # å¯é€‰ï¼š'w' è¦†ç›–å†™å…¥ï¼Œ'a' è¿½åŠ å†™å…¥
)
# =========================================================================


class SpinLock:
    def __init__(self):
        self.lock = threading.Lock()
    def acquire(self):
        while not self.lock.acquire(False):
            time.sleep(0.001)
    def release(self):
        self.lock.release()
    def __enter__(self):
        self.acquire()
        return self
    def __exit__(self, exc_type, exc_value, traceback):
        self.release()

global_lock = SpinLock()


def extract_json(response):
    match = re.search(r'\{.*\}', response, re.DOTALL)
    if match:
        try:
            return json.loads(match.group(0))
        except json.JSONDecodeError as e:
            logging.warning(f"JSON è§£æå¤±è´¥: {e}, è·³è¿‡è¯¥æ¡æ•°æ®: {response}")
            return None
    else:
        logging.warning(f"æœªæ‰¾åˆ° JSON æ•°æ®: {response}")
        return None

def clean_value(value):
    """ç®€å•æ¸…æ´—è¿”å›å€¼ï¼ˆå¦‚å»é™¤ä¸¤ç«¯ç©ºæ ¼ï¼‰"""
    return value.strip() if isinstance(value, str) else value

def query_partitions(sentence, k=3, threshold=0.4):
    """
    æ ¹æ®è¾“å…¥çš„å¥å­ï¼Œä½¿ç”¨ find_matches æŸ¥è¯¢è·å¾—å‰ k ä¸ªåŒ¹é…çš„æ–‡æœ¬åˆ‡ç‰‡ï¼Œ
    æ‹¼æ¥ç‰‡æ®µå†…å®¹å’Œå¯¹åº”é“¾æ¥å½¢æˆç´ æå­—ç¬¦ä¸²ã€‚
    """
    matches = find_matches([sentence], top_n=k, threshold=threshold)
    parts = ""
    if matches and matches[0]["matches"]:
        for match in matches[0]["matches"]:
            # match æ ¼å¼ï¼š(ç›¸ä¼¼åº¦, æ–‡æœ¬ç‰‡æ®µ, æ–‡ç« æ ‡é¢˜, zhihu_link)
            parts += f"ç´ æï¼š{match[1]} (é“¾æ¥ï¼š{match[3]})\n"
    return parts

def clean_text(text):
    logging.info("æ¸…æ´—æ–‡æœ¬")
    text = re.sub(r'https?://\S+', '', text)
    text = re.sub(r'!\[.*?\]\(.*?\.svg\)', '', text, flags=re.IGNORECASE)
    text = re.sub(r'<svg.*?>.*?</svg>', '', text, flags=re.DOTALL | re.IGNORECASE)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def safe_write_json(filepath, data):
    temp_filepath = filepath + ".tmp"
    with open(temp_filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
        f.flush()
        os.fsync(f.fileno())
    os.replace(temp_filepath, filepath)

def safe_write_csv(filepath, df):
    try:
        df.to_csv(filepath, index=False, encoding="utf-8")
    except Exception as e:
        logging.error("å†™å…¥ index.csv å¤±è´¥ï¼š%s", e)

def load_index():
    if os.path.exists(INDEX_FILE):
        logging.info("åŠ è½½ç´¢å¼•: %s", INDEX_FILE)
        df = pd.read_csv(INDEX_FILE)
        return df[["title", "url", "processed"]]
    else:
        logging.info("ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ ¹æ® articles.json åˆ›å»ºæ–°çš„ç´¢å¼•è¡¨")
        return pd.DataFrame(columns=["title", "url", "processed"])

def save_index(index_df):
    index_df["url"] = index_df["url"].apply(lambda x: x.rstrip() + " ")
    index_df = index_df[["title", "url", "processed"]]
    index_df["sort_key"] = index_df["processed"].apply(lambda x: 0 if x == PROCESSED else 1)
    index_df = index_df.sort_values(by=["sort_key"]).drop(columns=["sort_key"])
    safe_write_csv(INDEX_FILE, index_df)
    return index_df

def update_index_from_metadata(index_df, metadata_cache):
    logging.info("æ ¹æ® metadata_cache æ›´æ–° index çŠ¶æ€")
    processed_urls = { entry.get("url", "").strip() for entry in metadata_cache.values() if entry.get("url", "").strip() }
    index_df["url_stripped"] = index_df["url"].apply(lambda x: x.strip())
    index_df["processed"] = index_df["url_stripped"].apply(lambda x: PROCESSED if x in processed_urls else UNPROCESSED)
    index_df.drop(columns=["url_stripped"], inplace=True)
    index_df = save_index(index_df)
    logging.info("index æ›´æ–°å®Œæ¯•ï¼Œå…±æ›´æ–° %d ä¸ªæ¡ç›®", len(processed_urls))
    return index_df

def load_metadata_cache():
    logging.info("åŠ è½½å…ƒæ•°æ®ç¼“å­˜: %s", METADATA_FILE)
    if os.path.exists(METADATA_FILE):
        try:
            with open(METADATA_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            logging.error("è§£æ %s å¤±è´¥: %s", METADATA_FILE, e)
            backup_file = METADATA_FILE + ".bak"
            os.rename(METADATA_FILE, backup_file)
            logging.info("å·²å°†æŸåçš„æ–‡ä»¶å¤‡ä»½åˆ°: %s", backup_file)
            return {}
    return {}

def load_embedding_cache():
    logging.info("åŠ è½½åµŒå…¥å‘é‡ç¼“å­˜: %s", EMBEDDING_FILE)
    if os.path.exists(EMBEDDING_FILE):
        try:
            with open(EMBEDDING_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            logging.error("è§£æ %s å¤±è´¥: %s", EMBEDDING_FILE, e)
            backup_file = EMBEDDING_FILE + ".bak"
            os.rename(EMBEDDING_FILE, backup_file)
            logging.info("å·²å°†æŸåçš„æ–‡ä»¶å¤‡ä»½åˆ°: %s", backup_file)
            return {}
    return {}

def save_metadata_cache():
    safe_write_json(METADATA_FILE, metadata_cache)

def save_embedding_cache():
    safe_write_json(EMBEDDING_FILE, embedding_vectors)

def save_cache():
    save_metadata_cache()
    save_embedding_cache()

def load_system_metadata_cache():
    logging.info("åŠ è½½ system çº§åˆ«çš„å…ƒæ•°æ®ç¼“å­˜: %s", SYSTEM_METADATA_FILE)
    if os.path.exists(SYSTEM_METADATA_FILE):
        try:
            with open(SYSTEM_METADATA_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError as e:
            logging.error("è§£æ %s å¤±è´¥: %s", SYSTEM_METADATA_FILE, e)
            backup_file = SYSTEM_METADATA_FILE + ".bak"
            os.rename(SYSTEM_METADATA_FILE, backup_file)
            logging.info("å·²å°†æŸåçš„æ–‡ä»¶å¤‡ä»½åˆ°: %s", backup_file)
            return {}
    return {}

def load_system_embedding_cache():
    logging.info("åŠ è½½ system çº§åˆ«çš„åµŒå…¥å‘é‡ç¼“å­˜: %s", SYSTEM_EMBEDDING_FILE)
    if os.path.exists(SYSTEM_EMBEDDING_FILE):
        try:
            with open(SYSTEM_EMBEDDING_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
            return {k: np.array(v) for k, v in data.items()}
        except json.JSONDecodeError as e:
            logging.error("è§£æ %s å¤±è´¥: %s", SYSTEM_EMBEDDING_FILE, e)
            backup_file = SYSTEM_EMBEDDING_FILE + ".bak"
            os.rename(SYSTEM_EMBEDDING_FILE, backup_file)
            logging.info("å·²å°†æŸåçš„æ–‡ä»¶å¤‡ä»½åˆ°: %s", backup_file)
            return {}
    return {}

def save_system_metadata_cache():
    safe_write_json(SYSTEM_METADATA_FILE, system_metadata_cache)

def save_system_embedding_cache():
    system_embedding_vectors_serializable = {k: v.tolist() if isinstance(v, np.ndarray) else v 
                                             for k, v in system_embedding_vectors.items()}
    safe_write_json(SYSTEM_EMBEDDING_FILE, system_embedding_vectors_serializable)
    logging.info(f"system_embedding_vectors.json å·²æˆåŠŸä¿å­˜ï¼Œå…± {len(system_embedding_vectors)} æ¡æ•°æ®")

metadata_cache = load_metadata_cache()
embedding_vectors = load_embedding_cache()
system_metadata_cache = load_system_metadata_cache()
system_embedding_vectors = load_system_embedding_cache()

def build_index_from_articles(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    total_articles = len(data.get("articles", []))
    effective_articles = []
    for article in data["articles"]:
        if "æƒ³æ³•é›†" in article.get("tags", []):
            continue
        title = article.get("title", "")
        if not title:
            continue
        zhihu_link = article.get("zhihuLink", "").strip()
        if not zhihu_link or zhihu_link.startswith("ID_"):
            continue
        effective_articles.append({
            "title": title,
            "url": zhihu_link,
            "processed": UNPROCESSED
        })
    effective_count = len(effective_articles)
    df = pd.DataFrame(effective_articles)
    logging.info("ç´¢å¼•è¡¨ä¸­æ€»æ–‡ç« æ•°é‡ï¼š%d", effective_count)
    return total_articles, effective_count, df

def hash_sentence(text):
    return hashlib.md5(text.encode()).hexdigest()

def split_text_into_chunks(text, chunk_size=CHUNK_SIZE, overlap=10):
    tokens = list(jieba.lcut(text))
    chunks = []
    if not tokens:
        logging.warning("æ–‡æœ¬åˆ†è¯ç»“æœä¸ºç©º")
        return chunks
    step = chunk_size - overlap
    i = 0
    while i < len(tokens):
        chunk = tokens[i:i + chunk_size]
        chunks.append("".join(chunk))
        i += step
    return chunks

def clean_and_prepare_text(content):
    content = clean_text(content)
    text = " ".join([line for line in content.split('\n') if not line.startswith("![]") and not line.startswith("```")])
    return text

def get_batch_embeddings(chunks, article_titles, urls, provider="zhipu"):
    global metadata_cache, embedding_vectors

    logging.info("è·å– batch åµŒå…¥ï¼Œå…± %d ä¸ªåˆ‡ç‰‡", len(chunks))
    batch_hashes = [hash_sentence(c) for c in chunks]
    batch_chunks, batch_titles, batch_urls = [], [], []
    filtered_hashes = []

    for i, h in enumerate(batch_hashes):
        if h not in embedding_vectors:
            filtered_hashes.append(h)
            batch_chunks.append(chunks[i])
            batch_titles.append(article_titles[i])
            batch_urls.append(urls[i])

    if not batch_chunks:
        logging.info("æ‰€æœ‰åˆ‡ç‰‡å‡å·²è®¡ç®—ï¼Œæ— éœ€è¯·æ±‚ API")
        return

    max_retries = 3
    retry_delay = 5

    for attempt in range(max_retries):
        try:
            logging.info("è¯·æ±‚åµŒå…¥ APIï¼ˆ%sï¼‰ï¼Œå°è¯•æ¬¡æ•° %d", provider, attempt + 1)

            if provider == "zhipu":
                # è°ƒç”¨ Zhipu SDK
                response = zhipu_client.embeddings.create(
                    model=EMBEDDING_MODEL,
                    input=batch_chunks
                )
                if "data" in response and response["data"]:
                    sorted_data = sorted(response["data"], key=lambda x: x["index"])
                    embeddings = [item["embedding"] for item in sorted_data]
                else:
                    logging.warning("Zhipu è¿”å›ä¸ºç©º")
                    return

            elif provider == "openai":
                # ä½¿ç”¨ requests è°ƒç”¨ OpenAIï¼ˆç¤ºä¾‹ï¼‰
                headers = {
                    "Authorization": f"Bearer {API_KEY}",
                    "Content-Type": "application/json"
                }
                data = {
                    "input": batch_chunks,
                    "model": EMBEDDING_MODEL
                }
                response = requests.post(API_URL, headers=headers, json=data, timeout=30)
                response.raise_for_status()
                response_json = response.json()
                if "data" in response_json and response_json["data"]:
                    sorted_data = sorted(response_json["data"], key=lambda x: x["index"])
                    embeddings = [item["embedding"] for item in sorted_data]
                else:
                    logging.warning("OpenAI è¿”å›ä¸ºç©º")
                    return

            else:
                logging.error("ä¸æ”¯æŒçš„ provider: %s", provider)
                return

            for i, h in enumerate(filtered_hashes):
                embedding_vectors[h] = embeddings[i] if i < len(embeddings) else []
                metadata_cache[h] = {
                    "sentence": batch_chunks[i],
                    "article": batch_titles[i],
                    "url": batch_urls[i].strip()
                }
            save_cache()
            logging.info("Batch åµŒå…¥æˆåŠŸ")
            return

        except Exception as e:
            logging.warning("è¯·æ±‚å¤±è´¥: %s, æ­£åœ¨é‡è¯• (%d/%d)...", e, attempt + 1, max_retries)
            time.sleep(retry_delay)

    logging.error("API è¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡è¯¥ batch")

def get_embedding(text, article_title, url, is_query=False):
    global metadata_cache, embedding_vectors, system_metadata_cache, system_embedding_vectors
    text_hash = hash_sentence(text)
    if is_query:
        embedding_cache = system_embedding_vectors
        metadata_cache_ref = system_metadata_cache
        save_metadata_cache_func = save_system_metadata_cache
        save_embedding_cache_func = save_system_embedding_cache
    else:
        embedding_cache = embedding_vectors
        metadata_cache_ref = metadata_cache
        save_metadata_cache_func = save_metadata_cache
        save_embedding_cache_func = save_embedding_cache
    with global_lock:
        if text_hash in embedding_cache:
            logging.info(f"ä½¿ç”¨ç¼“å­˜ä¸­çš„åµŒå…¥: {text[:30]}")
            return embedding_cache[text_hash]
    headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}
    data = {"input": text, "model": MODEL_NAME}
    max_retries = 3
    retry_delay = 5
    for attempt in range(max_retries):
        try:
            logging.info(f"è¯·æ±‚ OpenAI API è·å–åµŒå…¥ (å°è¯• {attempt+1}/{max_retries}): {text[:30]}")
            response = requests.post(API_URL, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            response_json = response.json()
            if "data" in response_json and response_json["data"]:
                embedding = response_json["data"][0]["embedding"]
                with global_lock:
                    embedding_cache[text_hash] = embedding
                    metadata_cache_ref[text_hash] = {
                        "sentence": text,
                        "article": article_title if not is_query else "é¢„è®¾ query",
                        "url": url.strip() if not is_query else "#"
                    }
                    save_metadata_cache_func()
                    save_embedding_cache_func()
                logging.info(f"æ–°åµŒå…¥å·²å­˜å…¥ {'system' if is_query else 'æ–‡ç« '} ç¼“å­˜: {text[:30]}")
                return embedding
            else:
                logging.error(f"API è¿”å›é”™è¯¯: {response_json}, è·³è¿‡è¯¥æ–‡æœ¬")
                return None
        except requests.exceptions.RequestException as e:
            logging.warning(f"ç½‘ç»œé”™è¯¯: {e}, æ­£åœ¨é‡è¯• ({attempt+1}/{max_retries})...")
            time.sleep(retry_delay)
    logging.error(f"è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡è¯¥æ–‡æœ¬: {text[:30]}")
    return None

def get_batch_embeddings(texts, article_title, url, is_query=False, batch_size=64):
    global metadata_cache, embedding_vectors, system_metadata_cache, system_embedding_vectors

    if is_query:
        embedding_cache = system_embedding_vectors
        metadata_cache_ref = system_metadata_cache
        save_metadata_cache_func = save_system_metadata_cache
        save_embedding_cache_func = save_system_embedding_cache
    else:
        embedding_cache = embedding_vectors
        metadata_cache_ref = metadata_cache
        save_metadata_cache_func = save_metadata_cache
        save_embedding_cache_func = save_embedding_cache

    # æå–æœªç¼“å­˜çš„æ–‡æœ¬åŠå…¶ hash
    new_texts, new_text_hashes = [], []
    for text in texts:
        h = hash_sentence(text)
        with global_lock:
            if h not in embedding_cache:
                new_texts.append(text)
                new_text_hashes.append(h)

    if not new_texts:
        logging.info("æ‰€æœ‰åµŒå…¥å‡å·²å­˜åœ¨äºç¼“å­˜ä¸­")
        return [embedding_cache[hash_sentence(t)] for t in texts]

    # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š 64 ä¸ª
    for i in range(0, len(new_texts), batch_size):
        batch_texts = new_texts[i:i + batch_size]
        batch_hashes = new_text_hashes[i:i + batch_size]

        headers = {"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"}
        data = {"input": batch_texts, "model": MODEL_NAME}
        max_retries = 3
        retry_delay = 5

        for attempt in range(max_retries):
            try:
                logging.info(f"è¯·æ±‚ OpenAI API è·å–åµŒå…¥ (å°è¯• {attempt+1}/{max_retries})ï¼Œæœ¬æ‰¹ %d æ¡", len(batch_texts))
                response = requests.post(API_URL, headers=headers, json=data, timeout=60)
                response.raise_for_status()
                response_json = response.json()

                if "data" in response_json:
                    with global_lock:
                        for item in response_json["data"]:
                            h = batch_hashes[item["index"]]
                            emb = item["embedding"]
                            embedding_cache[h] = emb
                            metadata_cache_ref[h] = {
                                "sentence": batch_texts[item["index"]],
                                "article": article_title if not is_query else "é¢„è®¾ query",
                                "url": url.strip() if not is_query else "#"
                            }
                    break  # æœ¬æ‰¹æˆåŠŸï¼Œè·³å‡º retry å¾ªç¯
                else:
                    logging.error(f"API è¿”å›æ ¼å¼é”™è¯¯: {response_json}")
                    break
            except requests.exceptions.RequestException as e:
                logging.warning(f"ç½‘ç»œé”™è¯¯: {e}, æ­£åœ¨é‡è¯• ({attempt+1}/{max_retries})...")
                time.sleep(retry_delay)
        else:
            logging.error("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæœ¬æ‰¹å¤±è´¥")

    # ä¿å­˜å…¨éƒ¨ç¼“å­˜
    with global_lock:
        save_metadata_cache_func()
        save_embedding_cache_func()

    # è¿”å›å®Œæ•´åµŒå…¥åˆ—è¡¨ï¼ˆå«å·²æœ‰çš„ + æœ¬æ¬¡æ–°å¢çš„ï¼‰
    return [embedding_cache.get(hash_sentence(t)) for t in texts]


def process_sentences(chunks, article_title, url):
    logging.info("å¤„ç†æ–‡ç« åˆ‡ç‰‡ï¼ˆå¹¶å‘æ¨¡å¼ï¼‰ï¼Œæ–‡ç« æ ‡é¢˜: %s, åˆ‡ç‰‡æ•°é‡: %d", article_title, len(chunks))
    filtered_chunks = []
    for chunk in chunks:
        h = hash_sentence(chunk)
        with global_lock:
            if h in embedding_vectors:
                continue
        filtered_chunks.append(chunk)
    if not filtered_chunks:
        logging.info("æ‰€æœ‰åˆ‡ç‰‡å‡å·²è®¡ç®—ï¼Œæ— éœ€å†æ¬¡å¤„ç†")
        return

    if BATCH_REQUEST_TO_ZHIPU:
        _ = get_batch_embeddings(filtered_chunks, article_title, url, is_query=False)
    else:
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKER) as executor:
            futures = [executor.submit(get_embedding, chunk, article_title, url, is_query=False) for chunk in filtered_chunks]
            for future in concurrent.futures.as_completed(futures):
                _ = future.result()
    

def load_articles(file_path, sample_ratio=0.1, sample_count=None):
    logging.info("åŠ è½½æ–‡ç« æ–‡ä»¶: %s", file_path)
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    total_articles = len(data.get("articles", []))
    logging.info("å½“å‰æ€»æ–‡ç« æ•°é‡: %d", total_articles)
    valid_urls_set = set()
    for article in data["articles"]:
        if "æƒ³æ³•é›†" in article.get("tags", []):
            continue
        title = article.get("title", "")
        if not title:
            continue
        zhihu_link = article.get("zhihuLink", "").strip()
        if not zhihu_link or zhihu_link.startswith("ID_"):
            continue
        valid_urls_set.add(zhihu_link)
    logging.info("å½“å‰æœ‰æ•ˆæ–‡ç« æ•°é‡: %d", len(valid_urls_set))
    index_df = load_index()
    if index_df.empty:
        _, effective_count, index_df = build_index_from_articles(file_path)
        logging.info("ç´¢å¼•è¡¨ä¸­å…±æœ‰ %d ç¯‡æ–‡ç« ", effective_count)
        index_df = save_index(index_df)
    else:
        index_df = index_df[index_df["url"].apply(lambda x: x.strip()).isin(valid_urls_set)]
    index_df = update_index_from_metadata(index_df, metadata_cache)
    logging.info("å½“å‰å·²å»ºç«‹ç´¢å¼•çš„æ–‡ç« æ•°é‡: %d", len(index_df))
    new_entries = []
    for article in data["articles"]:
        if "æƒ³æ³•é›†" in article.get("tags", []):
            continue
        title = article.get("title", "")
        if not title:
            continue
        zhihu_link = article.get("zhihuLink", "").strip()
        if not zhihu_link or zhihu_link.startswith("ID_"):
            continue
        if zhihu_link not in index_df["url"].apply(lambda x: x.strip()).values:
            new_entries.append({"title": title, "url": zhihu_link, "processed": UNPROCESSED})
    if new_entries:
        logging.info("æ–°å¢ %d ç¯‡æ–‡ç« åˆ°ç´¢å¼•", len(new_entries))
        index_df = pd.concat([index_df, pd.DataFrame(new_entries)], ignore_index=True)
        index_df = save_index(index_df)
    unprocessed_articles = index_df[index_df["processed"] == UNPROCESSED]
    if unprocessed_articles.empty:
        logging.info("æ‰€æœ‰æ–‡ç« å·²å¤„ç†ï¼Œæ— éœ€é‡‡æ ·ã€‚")
        return [], index_df
    if sample_count is not None:
        sample_size = sample_count
    else:
        sample_size = max(1, int(len(unprocessed_articles) * sample_ratio))
    logging.info("éšæœºé‡‡æ · %d ç¯‡æœªå¤„ç†çš„æ–‡ç« ...", sample_size)
    sampled_articles = unprocessed_articles.sample(n=sample_size)
    articles = []
    for _, row in tqdm(sampled_articles.iterrows(), total=sample_size, desc="åŠ è½½é‡‡æ ·æ–‡ç« ", file=sys.stdout):
        article = next(filter(lambda a: a.get("zhihuLink", "").strip() == row["url"].strip(), data["articles"]), None)
        if article is None:
            logging.warning("æœªåŒ¹é…åˆ°æ–‡ç« ï¼ŒURL: %s", row["url"])
            index_df.loc[index_df["url"].apply(lambda x: x.strip()) == row["url"].strip(), "processed"] = "æœªæ‰¾åˆ°"
            continue
        content = article.get("content", "")
        content = clean_text(content)
        text = " ".join([line for line in content.split('\n') if not line.startswith("![]") and not line.startswith("```")])
        chunk_list = split_text_into_chunks(text)
        process_sentences(chunk_list, article["title"], article.get("zhihuLink", "").strip())
        articles.append({
            "id": article["id"],
            "title": article["title"],
            "url": article.get("zhihuLink", "").strip(),
            "chunks": chunk_list,
            "content": content
        })
        index_df.loc[index_df["url"].apply(lambda x: x.strip()) == row["url"].strip(), "processed"] = PROCESSED
    index_df = save_index(index_df)
    logging.info("é‡‡æ ·æ–‡ç« åŠ è½½å¹¶å¤„ç†å®Œæ¯•")
    

def process_articles(valid_articles, index_df):
    unprocessed_articles = index_df[index_df["processed"] == UNPROCESSED]
    if unprocessed_articles.empty:
        logging.info("æ‰€æœ‰æ–‡ç« å·²å¤„ç†ï¼Œæ— éœ€è¿›ä¸€æ­¥é‡‡æ ·ã€‚")
        return index_df
    logging.info("æŒ‰ batch=%d å¼€å§‹å¤„ç†æ–‡ç« ...", BATCH_SIZE)
    batches = []
    for i in range(0, len(unprocessed_articles), BATCH_SIZE):
        batches.append(unprocessed_articles.iloc[i:i + BATCH_SIZE])
    
    def process_batch(batch_articles):
        chunks_list, titles, urls = [], [], []
        processed_urls_in_batch = []
        for _, row in batch_articles.iterrows():
            article = next(filter(lambda a: (a.get("zhihuLink", "").strip() if a.get("zhihuLink", "").strip() 
                                             else f"ID_{a.get('id', a.get('title'))}") == row["url"].strip(), valid_articles), None)
            if not article:
                logging.warning("åœ¨æœ‰æ•ˆæ–‡ç« ä¸­æœªåŒ¹é…åˆ°ï¼ŒURL: %s", row["url"])
                index_df.loc[index_df["url"].apply(lambda x: x.strip()) == row["url"].strip(), "processed"] = "æœªæ‰¾åˆ°"
                continue
            content = article.get("content", "")
            content = clean_text(content)
            text = " ".join([line for line in content.split('\n') if not line.startswith("![]") and not line.startswith("```")])
            chunk_list = split_text_into_chunks(text)
            if chunk_list:
                chunks_list.extend(chunk_list)
                titles.extend([article["title"]] * len(chunk_list))
                urls.extend([(article.get("zhihuLink", "").strip() if article.get("zhihuLink", "").strip() 
                              else f"ID_{article.get('id', article['title'])}")] * len(chunk_list))
                processed_urls_in_batch.append(row["url"].strip())
        logging.info("å½“å‰ batch å…± %d ä¸ªåˆ‡ç‰‡ï¼Œå¼€å§‹è°ƒç”¨ API", len(chunks_list))
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            for j in range(0, len(chunks_list), SENTENCE_BATCH_SIZE):
                batch_chunk = chunks_list[j:j + SENTENCE_BATCH_SIZE]
                batch_titles = titles[j:j + SENTENCE_BATCH_SIZE]
                batch_urls = urls[j:j + SENTENCE_BATCH_SIZE]
                futures.append(executor.submit(get_batch_embeddings, batch_chunk, batch_titles, batch_urls))
            for future in concurrent.futures.as_completed(futures):
                _ = future.result()
        index_df.loc[index_df["url"].apply(lambda x: x.strip()).isin(processed_urls_in_batch), "processed"] = PROCESSED
        save_index(index_df)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as pool:
        pool.map(process_batch, batches)
    
    logging.info("æ‰€æœ‰æ–‡ç« æ‰¹é‡å¤„ç†å®Œæ¯•")
    return index_df

def find_matches(leaf_nodes, top_n=10, threshold=0.6):
    global system_embedding_vectors, system_metadata_cache
    logging.info("å¼€å§‹è¿›è¡Œ query åŒ¹é…ï¼Œå…± %d ä¸ª query", len(leaf_nodes))
    matches = []
    for query in tqdm(leaf_nodes, desc="è®¡ç®— Query åµŒå…¥", file=sys.stdout):
        query_hash = hash_sentence(query)
        if query_hash in system_embedding_vectors:
            logging.info(f"ä½¿ç”¨ system ç¼“å­˜ä¸­çš„ query åµŒå…¥: {query}")
            query_embedding = np.array(system_embedding_vectors[query_hash])
        else:
            query_embedding = np.array(get_embedding(query, "æœªçŸ¥æ ‡é¢˜", "#", is_query=True))
            if query_embedding is not None:
                system_embedding_vectors[query_hash] = query_embedding
                system_metadata_cache[query_hash] = {
                    "sentence": query,
                    "article": "é¢„è®¾ query",
                    "url": "#"
                }
                save_system_metadata_cache()
                save_system_embedding_cache()
                logging.info(f"æ–°çš„ query åµŒå…¥å·²å­˜å…¥ system ç¼“å­˜: {query}")
        similarities = []
        for text_hash, vector in tqdm(embedding_vectors.items(), desc="è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—", leave=False, file=sys.stdout):
            sim = cosine_similarity([query_embedding], [np.array(vector)])[0][0]
            meta = metadata_cache.get(text_hash, {})
            similarities.append((sim, meta.get("sentence", ""), meta.get("article", ""), meta.get("url", "")))
        sorted_results = sorted(similarities, key=lambda x: x[0], reverse=True)
        filtered_results = [item for item in sorted_results if item[0] > threshold][:top_n]
        if filtered_results:
            matches.append({"query": query, "matches": filtered_results})
    logging.info("query åŒ¹é…å®Œæˆ")
    return matches

def save_results(matches, processed_count, total_slices, top_n, threshold):
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    os.makedirs(f"{OUTPUT_DIR}/{timestamp}", exist_ok=True)
    result_filename = f"{OUTPUT_DIR}/{timestamp}/{processed_count}ç¯‡{total_slices}åˆ‡ç‰‡ä¸­æ’åå‰{top_n}ç›¸ä¼¼åº¦å¤§äº{threshold}çš„æ£€ç´¢ç»“æœ.md"
    logging.info("ä¿å­˜æ£€ç´¢ç»“æœåˆ°: %s", result_filename)
    with open(result_filename, "w", encoding="utf-8") as f:
        f.write("# åŒ¹é…ç»“æœ\n\n")
        for match in matches:
            f.write(f"## Query: {match['query']}\n")
            f.write("ğŸ” æ£€ç´¢åˆ°ä»¥ä¸‹åŒ¹é…ç‰‡æ®µï¼š\n\n")
            for sim, sentence, title, url in match["matches"]:
                f.write(f"- **{title}** (ç›¸ä¼¼åº¦: {sim:.2f})\n")
                f.write(f"  - URL: [{url}]({url})\n")
                f.write(f"  - åŒ¹é…ç‰‡æ®µ: {sentence}\n\n")
    logging.info("æ£€ç´¢ç»“æœä¿å­˜å®Œæ¯•")

def extract_leaf_nodes(tree):
    leaves = []
    if isinstance(tree, dict):
        for key, value in tree.items():
            leaves.extend(extract_leaf_nodes(value))
    elif isinstance(tree, list):
        for item in tree:
            leaves.extend(extract_leaf_nodes(item))
    else:
        leaves.append(tree)
    return leaves

def new_business_run(df):
    """
    å¯¹è¾“å…¥ DataFrame ä¸­çš„æ¯ä¸€è¡Œè¿›è¡Œå¤„ç†ï¼Œç”Ÿæˆå†…å¿ƒæ—ç™½ï¼Œ
    å¹¶è¿”å›ç”ŸæˆæˆåŠŸçš„ç»“æœåˆ—è¡¨åŠå¤±è´¥çš„å¥å­åˆ—è¡¨ï¼ˆåŸå§‹æ•°æ®è¡Œï¼‰ã€‚
    """
    results = []
    fails = []
    for idx, row in df.iterrows():
        sentence = row["è‡ªæˆ‘è‚¯å®šè¯­"]
        # è·å–å¤šä¸ªç´ æç‰‡æ®µä½œä¸ºç”Ÿæˆä¾æ®
        parts_material = query_partitions(sentence, k=3, threshold=0.6)
        prompt = f"""
è‡ªæˆ‘è‚¯å®šè¯­ï¼š{sentence}

è¯·ä½¿ç”¨è¿™äº›ç´ æ<parts>{parts_material}</parts>ï¼Œä¸ºè¾“å…¥çš„è‡ªæˆ‘è‚¯å®šè¯­ç”Ÿæˆä¸€æ®µå†…å¿ƒæ—ç™½ã€‚
æ³¨æ„é€‚å½“æ¢è¡Œä»¥å‡å°‘è¯»è€…çš„é˜…è¯»éš¾åº¦ã€‚åˆ†ä¸‰åˆ°å››æ®µç”Ÿæˆå†…å¿ƒæ—ç™½ã€‚ä¸è¦å†™è¯—ã€‚
çº¦500å­—ã€‚
å¿…é¡»ä»¥ç¬¬ä¸€äººç§°å™è¿°ã€‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¿”å›æ•°æ®ï¼š
{{
"inner_monologue": "è¿™é‡Œæ˜¯ç”Ÿæˆçš„å†…å¿ƒæ—ç™½å†…å®¹"
}}
"""
        try:
            messages = [{"role": "user", "content": prompt}]
            response = send_messages(messages, api_key=KIMI_API_KEY)
            parsed_response = extract_json(response)
            if parsed_response and "inner_monologue" in parsed_response:
                check_prompt = f"""
é’ˆå¯¹ä¸Šä¸€æ¬¡ç”Ÿæˆçš„å†…å¿ƒæ—ç™½ï¼š
{parsed_response["inner_monologue"]}

è¯·æ£€æŸ¥å¹¶ä¼˜åŒ–ä»¥ä¸‹å†…å®¹ï¼š
- ä¿®æ­£æ ‡ç‚¹/ç©ºæ ¼é—®é¢˜
- æ”¹å–„è¯­å¥é€šé¡ºåº¦
- ç»Ÿä¸€äººç§°ï¼ˆç¬¬ä¸€äººç§°ï¼‰ï¼Œå¿…é¡»ä»¥ç¬¬ä¸€äººç§°å™è¿°ã€‚
- åˆ é™¤å¤–è¯­å†…å®¹
- é˜²æ­¢åœºæ™¯è¿‡äºå…·ä½“
- ç¡®ä¿500å­—é•¿åº¦
- åˆ é™¤å¥‡æ€ªæ¯”å–»
- ä¿®æ­£è¯­ç—…/é”™åˆ«å­—

ç›´æ¥è¿”å›ä¼˜åŒ–åçš„JSONï¼š
{{
    "inner_monologue": "è¿™é‡Œæ˜¯ä¿®æ”¹åç”Ÿæˆçš„å†…å¿ƒæ—ç™½å†…å®¹"
}}
"""
                messages.append({"role": "user", "content": check_prompt})
                check_response = send_messages(messages, api_key=KIMI_API_KEY)
                check_parsed_response = extract_json(check_response)
                paragraphs = check_parsed_response["inner_monologue"].split("\n")
                valid = True
                for para in paragraphs:
                    if len(para) > 300:
                        logging.warning(f"å•æ®µè½è¶…è¿‡300å­—ï¼Œè·³è¿‡ï¼š{para}")
                        valid = False
                        break
                if not valid:
                    fails.append(row)
                    continue
                if check_parsed_response and "inner_monologue" in check_parsed_response:
                    inner_monologue = clean_value(check_parsed_response["inner_monologue"])
                    # ä½¿ç”¨ç”Ÿæˆçš„å†…å¿ƒæ—ç™½è¿›è¡Œæ£€ç´¢ï¼Œè·å–å‰ 3 ä¸ªå‚è€ƒç‰‡æ®µå’Œå¯¹åº”çš„ zhihu_link
                    retrieval_results = find_matches([inner_monologue], top_n=3, threshold=0.6)
                    reference_sources = []
                    if retrieval_results and retrieval_results[0]["matches"]:
                        for match in retrieval_results[0]["matches"]:
                            reference_sources.append({
                                "snippet": match[1],
                                "zhihu_link": match[3]
                            })
                    result = {
                        "è‡ªæˆ‘è‚¯å®šè¯­": sentence,
                        "è‡ªæˆ‘æ—ç™½": inner_monologue,
                        "MODEL_NAME": MODEL_NAME,
                        "å‚è€ƒæº": json.dumps(reference_sources, ensure_ascii=False)
                    }
                    results.append(result)
                else:
                    logging.warning(f"ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡: {sentence}")
                    fails.append(row)
            else:
                logging.warning(f"ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡: {sentence}")
                fails.append(row)
        except Exception as e:
            logging.error(f"å¤„ç† {sentence} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            fails.append(row)
    return results, fails

def make_inner_monologue_by_kimi(mode=None):
    """
    ä» CSV ä¸­æŠ½æ ·å¥å­ç”Ÿæˆå†…å¿ƒæ—ç™½ï¼Œå¹¶ä¿å­˜ç”Ÿæˆç»“æœï¼›
    å¯¹ç”Ÿæˆå¤±è´¥çš„å¥å­ä¿å­˜åˆ° fail.csv ä¸­ï¼Œä¸”æ”¯æŒé‡è¯•ã€‚
    """
    if not mode:
        while True:
            print("è¯·é€‰æ‹©æ–°ä¸šåŠ¡å¤„ç†æ¨¡å¼ï¼š")
            print("1. ä»å¤´å¼€å§‹")
            print("2. ä» fail.csv å¼€å§‹")
            mode = input("è¯·è¾“å…¥ 1 æˆ– 2ï¼š").strip()
            if mode == "1":
                try:
                    df = pd.read_csv("./data/0315å¥å­æ›´æ–° - æ±‡æ€»è¡¨.csv").sample(5)
                except Exception as e:
                    logging.error("è¯»å– CSV æ–‡ä»¶å¤±è´¥: %s", e)
                    return
                break
            elif mode == "2":
                if os.path.exists(FAIL_FILE):
                    df = pd.read_csv(FAIL_FILE)
                    if df.empty:
                        print("./cache/fail.csv ä¸­æ²¡æœ‰å¤±è´¥çš„è®°å½•ï¼Œç¨‹åºé€€å‡ºã€‚")
                        return
                    break
                else:
                    print("./cache/fail.csv æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·é€‰æ‹©ä»å¤´å¼€å§‹ã€‚")
            else:
                print("è¾“å…¥æœ‰è¯¯ï¼Œè¯·é‡æ–°é€‰æ‹©ã€‚")
    else:
        if mode == "1":
            try:
                df = pd.read_csv("./data/0315å¥å­æ›´æ–° - æ±‡æ€»è¡¨.csv").sample(5)
            except Exception as e:
                logging.error("è¯»å– CSV æ–‡ä»¶å¤±è´¥: %s", e)
                return
        elif mode == "2":
            if os.path.exists(FAIL_FILE):
                df = pd.read_csv(FAIL_FILE)
                if df.empty:
                    print("./cache/fail.csv ä¸­æ²¡æœ‰å¤±è´¥çš„è®°å½•ï¼Œç¨‹åºé€€å‡ºã€‚")
                    return
            else:
                print("./cache/fail.csv æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·é€‰æ‹©ä»å¤´å¼€å§‹ã€‚")
                return
        else:
            print("è¾“å…¥æœ‰è¯¯ï¼Œè¯·é‡æ–°é€‰æ‹©ã€‚")
            return


    results, fails = new_business_run(df)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    result_filename = f"{OUTPUT_DIR}/{timestamp}-{cache_dir}.csv"
    df_results = pd.DataFrame(results)
    df_results.to_csv(result_filename, index=False, encoding='utf-8-sig')
    logging.info("æ–°ä¸šåŠ¡ç”Ÿæˆç»“æœå·²ä¿å­˜åˆ° CSV æ–‡ä»¶: %s", result_filename)
    print("æ–°ä¸šåŠ¡ç”Ÿæˆç»“æœå·²ä¿å­˜åˆ° CSV æ–‡ä»¶:", result_filename)

    # ä¿å­˜å¤±è´¥è®°å½•
    df_fail = pd.DataFrame(fails)
    df_fail.to_csv(FAIL_FILE, index=False, encoding='utf-8-sig')
    if not df_fail.empty:
        print(f"å…±æœ‰ {len(df_fail)} ä¸ªå¥å­ç”Ÿæˆå¤±è´¥ï¼Œå·²ä¿å­˜åˆ° fail.csvã€‚")
        retry = input("æ˜¯å¦ç«‹å³é‡è¯•è¿™äº›å¤±è´¥çš„è®°å½•ï¼Ÿ(Y/N): ").strip().lower()
        if retry == "y":
            # é€’å½’é‡è¯•
            make_inner_monologue_by_kimi(mode="2")
    else:
        print("å…¨éƒ¨ç”ŸæˆæˆåŠŸï¼Œæ— å¤±è´¥è®°å½•ã€‚")



# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(description="æ–‡ç« æ£€ç´¢ä¸åµŒå…¥å¤„ç†")
#     parser.add_argument("-q", "--question", type=str, help="è¾“å…¥å•ä¸ªæ£€ç´¢ query")
#     parser.add_argument("--debug", action="store_true", help="æ˜¯å¦æ˜¾ç¤º DEBUG ä¿¡æ¯")
#     args = parser.parse_args()
#     if args.debug:
#         logging.getLogger().setLevel(logging.DEBUG)
#         logging.debug("DEBUG æ¨¡å¼å·²å¼€å¯")

#     current_index = load_index()
#     if current_index.empty:
#         total_articles, effective_count, current_index = build_index_from_articles(ARTICLES_FILE)
#         logging.info("å½“å‰æ€»æ–‡ç« æ•°é‡: %d", total_articles)
#         logging.info("å½“å‰æœ‰æ•ˆæ–‡ç« æ•°é‡: %d", effective_count)
#         logging.info("ç´¢å¼•è¡¨ä¸­å…±æœ‰ %d ç¯‡æ–‡ç« ", effective_count)
#         current_index = save_index(current_index)
#     else:
#         logging.info("ç´¢å¼•è¡¨ä¸­å…±æœ‰ %d ç¯‡æ–‡ç« ", len(current_index))
#     current_index = update_index_from_metadata(current_index, metadata_cache)
#     total_index_count = len(current_index)
#     processed_count = len(current_index[current_index["processed"] == PROCESSED])
#     logging.info("å½“å‰å·²å»ºç«‹ç´¢å¼•çš„æ–‡ç« æ•°é‡: %d", total_index_count)
#     logging.info("å½“å‰å·²å»ºç«‹åˆ‡ç‰‡çš„æ–‡ç« æ•°é‡: %d", processed_count)
#     logging.info("å½“å‰æœªå»ºç«‹åˆ‡ç‰‡çš„æ–‡ç« æ•°é‡: %d", total_index_count - processed_count)
#     logging.info("å½“å‰æ€»åˆ‡ç‰‡æ•°é‡ï¼š%d", len(embedding_vectors))
    
#     if args.question:
#         query = args.question
#         logging.info("æ”¶åˆ°æ£€ç´¢ query: %s", query)
#         try:
#             top_n = int(input("è¯·è¾“å…¥ä¿ç•™çš„åŒ¹é…æ•°é‡ï¼ˆé»˜è®¤10ï¼‰: ").strip() or 10)
#         except:
#             top_n = 10
#         try:
#             threshold = float(input("è¯·è¾“å…¥ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.6ï¼‰: ").strip() or 0.6)
#         except:
#             threshold = 0.6
#         matches = find_matches([query], top_n=top_n, threshold=threshold)
#         save_results(matches, processed_count, len(embedding_vectors), top_n, threshold)
#     else:
#         print(f"{cache_dir}")
#         print("è¯·é€‰æ‹©åç»­æ“ä½œ:")
#         print("1. æ·»åŠ æ–°çš„åˆ‡ç‰‡å†…å®¹ï¼ˆå¤„ç†æ–°æ–‡ç« ï¼‰")
#         print("2. ç›´æ¥è¿›è¡ŒæŸ¥è¯¢")
#         print("3. ç”Ÿæˆè‡ªæˆ‘æ—ç™½ï¼ˆæ–°ä¸šåŠ¡ï¼‰")
#         choice = input("è¯·è¾“å…¥ 1, 2 æˆ– 3ï¼š").strip()
#         if choice == "1":
#             logging.info("ç”¨æˆ·é€‰æ‹©æ·»åŠ æ–°çš„åˆ‡ç‰‡å†…å®¹")
#             current_index = load_index()
#             unprocessed_articles = current_index[current_index["processed"] == UNPROCESSED]
#             unprocessed_count = len(unprocessed_articles)
#             print(f"å½“å‰è¿˜æœ‰ {unprocessed_count} ç¯‡æ–‡ç« æœªå¤„ç†ã€‚")
#             while True:
#                 try:
#                     sample_count = int(input("è¯·è¾“å…¥è¦æ·»åŠ å¤„ç†çš„æ–‡ç« æ•°é‡ï¼ˆä¸è¶…è¿‡æœªå¤„ç†æ–‡ç« æ•°é‡ï¼‰ï¼š").strip())
#                     if 1 <= sample_count <= unprocessed_count:
#                         break
#                     else:
#                         print(f"è¯·è¾“å…¥ä¸€ä¸ªä»‹äº 1 åˆ° {unprocessed_count} çš„æ•´æ•°ã€‚")
#                 except ValueError:
#                     print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ä¸€ä¸ªæ•´æ•°ã€‚")
#             valid_articles, index_df = load_articles(ARTICLES_FILE, sample_count=sample_count)
#             current_index = load_index()  # é‡æ–°åŠ è½½æ›´æ–°åçš„ç´¢å¼•
#             total_index_count = len(current_index)
#             processed_count = len(current_index[current_index["processed"] == PROCESSED])
#             logging.info("å½“å‰å·²å»ºç«‹ç´¢å¼•çš„æ–‡ç« æ•°é‡: %d", total_index_count)
#             logging.info("å½“å‰å·²å»ºç«‹åˆ‡ç‰‡çš„æ–‡ç« æ•°é‡: %d", processed_count)
#             try:
#                 top_n = int(input("è¯·è¾“å…¥ä¿ç•™çš„åŒ¹é…æ•°é‡ï¼ˆé»˜è®¤10ï¼‰: ").strip() or 10)
#             except:
#                 top_n = 10
#             try:
#                 threshold = float(input("è¯·è¾“å…¥ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.6ï¼‰: ").strip() or 0.6)
#             except:
#                 threshold = 0.6
#             sub_choice = input("è¯·é€‰æ‹©æŸ¥è¯¢æ¨¡å¼:\n1. è‡ªå®šä¹‰ query\n2. ä½¿ç”¨ system.json ä¸­é¢„è®¾çš„ query\nè¯·è¾“å…¥ 1 æˆ– 2ï¼š").strip()
#             if sub_choice == "1":
#                 query = input("è¯·è¾“å…¥æ£€ç´¢ query: ").strip()
#                 if query:
#                     matches = find_matches([query], top_n=top_n, threshold=threshold)
#                     save_results(matches, processed_count, len(embedding_vectors), top_n, threshold)
#                 else:
#                     logging.info("æœªè¿›è¡ŒæŸ¥è¯¢")
#             elif sub_choice == "2":
#                 logging.info("ä½¿ç”¨ system.json ä¸­é¢„è®¾çš„ query")
#                 try:
#                     with open(SYSTEM_FILE, "r", encoding="utf-8") as f:
#                         json_tree = json.load(f)
#                     preset_queries = extract_leaf_nodes(json_tree.get("å®¶æ—", {}))
#                     if preset_queries:
#                         matches = find_matches(preset_queries, top_n=top_n, threshold=threshold)
#                         save_results(matches, processed_count, len(embedding_vectors), top_n, threshold)
#                     else:
#                         logging.info("system.json ä¸­æ²¡æœ‰é¢„è®¾çš„ query")
#                 except Exception as e:
#                     logging.error("è¯»å– system.json å¤±è´¥: %s", e)
#             else:
#                 logging.info("æœªé€‰æ‹©ä»»ä½•æŸ¥è¯¢æ¨¡å¼ï¼Œç¨‹åºé€€å‡º")
#         elif choice == "2":
#             logging.info("ç”¨æˆ·é€‰æ‹©ç›´æ¥è¿›è¡ŒæŸ¥è¯¢")
#             sub_choice = input("è¯·é€‰æ‹©æŸ¥è¯¢æ¨¡å¼:\n1. è‡ªå®šä¹‰ query\n2. ä½¿ç”¨ system.json ä¸­é¢„è®¾çš„ query\nè¯·è¾“å…¥ 1 æˆ– 2ï¼š").strip()
#             try:
#                 top_n = int(input("è¯·è¾“å…¥ä¿ç•™çš„åŒ¹é…æ•°é‡ï¼ˆé»˜è®¤10ï¼‰: ").strip() or 10)
#             except:
#                 top_n = 10
#             try:
#                 threshold = float(input("è¯·è¾“å…¥ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.6ï¼‰: ").strip() or 0.6)
#             except:
#                 threshold = 0.6
#             if sub_choice == "1":
#                 query = input("è¯·è¾“å…¥æ£€ç´¢ query: ").strip()
#                 if query:
#                     matches = find_matches([query], top_n=top_n, threshold=threshold)
#                     save_results(matches, processed_count, len(embedding_vectors), top_n, threshold)
#                 else:
#                     logging.info("æœªè¿›è¡ŒæŸ¥è¯¢")
#             elif sub_choice == "2":
#                 logging.info("ä½¿ç”¨ system.json ä¸­é¢„è®¾çš„ query")
#                 try:
#                     with open(SYSTEM_FILE, "r", encoding="utf-8") as f:
#                         json_tree = json.load(f)
#                     preset_queries = extract_leaf_nodes(json_tree.get("å®¶æ—", {}))
#                     if preset_queries:
#                         matches = find_matches(preset_queries, top_n=top_n, threshold=threshold)
#                         save_results(matches, processed_count, len(embedding_vectors), top_n, threshold)
#                     else:
#                         logging.info("system.json ä¸­æ²¡æœ‰é¢„è®¾çš„ query")
#                 except Exception as e:
#                     logging.error("è¯»å– system.json å¤±è´¥: %s", e)
#             else:
#                 logging.info("æœªé€‰æ‹©ä»»ä½•æŸ¥è¯¢æ¨¡å¼ï¼Œç¨‹åºé€€å‡º")
#         elif choice == "3":
#             logging.info("ç”¨æˆ·é€‰æ‹©æ–°ä¸šåŠ¡ï¼šç”Ÿæˆè‡ªæˆ‘æ—ç™½")
#             make_inner_monologue_by_kimi()
#         else:
#             logging.info("æœªé€‰æ‹©ä»»ä½•æ“ä½œï¼Œç¨‹åºé€€å‡º")

import argparse
import logging
import json


def setup_logging(debug: bool):
    if debug:
        logging.getLogger().setLevel(logging.DEBUG)
        logging.debug("DEBUG æ¨¡å¼å·²å¼€å¯")


def load_or_build_index():
    current_index = load_index()
    if current_index.empty:
        total_articles, effective_count, current_index = build_index_from_articles(ARTICLES_FILE)
        logging.info("å½“å‰æ€»æ–‡ç« æ•°é‡: %d", total_articles)
        logging.info("å½“å‰æœ‰æ•ˆæ–‡ç« æ•°é‡: %d", effective_count)
        logging.info("ç´¢å¼•è¡¨ä¸­å…±æœ‰ %d ç¯‡æ–‡ç« ", effective_count)
        current_index = save_index(current_index)
    else:
        logging.info("ç´¢å¼•è¡¨ä¸­å…±æœ‰ %d ç¯‡æ–‡ç« ", len(current_index))
    return current_index


def update_and_log_index(current_index):
    current_index = update_index_from_metadata(current_index, metadata_cache)
    total_index_count = len(current_index)
    processed_count = len(current_index[current_index["processed"] == PROCESSED])
    logging.info("å½“å‰å·²å»ºç«‹ç´¢å¼•çš„æ–‡ç« æ•°é‡: %d", total_index_count)
    logging.info("å½“å‰å·²å»ºç«‹åˆ‡ç‰‡çš„æ–‡ç« æ•°é‡: %d", processed_count)
    logging.info("å½“å‰æœªå»ºç«‹åˆ‡ç‰‡çš„æ–‡ç« æ•°é‡: %d", total_index_count - processed_count)
    logging.info("å½“å‰æ€»åˆ‡ç‰‡æ•°é‡ï¼š%d", len(embedding_vectors))
    return processed_count


def handle_query(query: str, top_n: int, threshold: float, processed_count: int):
    logging.info("æ”¶åˆ°æ£€ç´¢ query: %s", query)
    matches = find_matches([query], top_n=top_n, threshold=threshold)
    save_results(matches, processed_count, len(embedding_vectors), top_n, threshold)


def add_new_slices():
    logging.info("ç”¨æˆ·é€‰æ‹©æ·»åŠ æ–°çš„åˆ‡ç‰‡å†…å®¹")
    current_index = load_index()
    unprocessed_articles = current_index[current_index["processed"] == UNPROCESSED]
    unprocessed_count = len(unprocessed_articles)
    print(f"å½“å‰è¿˜æœ‰ {unprocessed_count} ç¯‡æ–‡ç« æœªå¤„ç†ã€‚")

    sample_count = get_sample_count(unprocessed_count)
    load_articles(ARTICLES_FILE, sample_count=sample_count)
    current_index = load_index()
    
    top_n, threshold = get_top_n_threshold()

    sub_choice = input("è¯·é€‰æ‹©æŸ¥è¯¢æ¨¡å¼:\n1. è‡ªå®šä¹‰ query\n2. ä½¿ç”¨ system.json ä¸­é¢„è®¾çš„ query\nè¯·è¾“å…¥ 1 æˆ– 2ï¼š").strip()
    processed_count = len(current_index[current_index["processed"] == PROCESSED])
    if sub_choice == "1":
        query = input("è¯·è¾“å…¥æ£€ç´¢ query: ").strip()
        if query:
            handle_query(query, top_n, threshold, processed_count)
        else:
            logging.info("æœªè¿›è¡ŒæŸ¥è¯¢")
    elif sub_choice == "2":
        use_preset_queries(top_n, threshold, processed_count)
    else:
        logging.info("æœªé€‰æ‹©ä»»ä½•æŸ¥è¯¢æ¨¡å¼ï¼Œç¨‹åºé€€å‡º")


def use_preset_queries(top_n, threshold, processed_count):
    logging.info("ä½¿ç”¨ system.json ä¸­é¢„è®¾çš„ query")
    try:
        with open(SYSTEM_FILE, "r", encoding="utf-8") as f:
            json_tree = json.load(f)
        preset_queries = extract_leaf_nodes(json_tree.get("å®¶æ—", {}))
        if preset_queries:
            matches = find_matches(preset_queries, top_n=top_n, threshold=threshold)
            save_results(matches, processed_count, len(embedding_vectors), top_n, threshold)
        else:
            logging.info("system.json ä¸­æ²¡æœ‰é¢„è®¾çš„ query")
    except Exception as e:
        logging.error("è¯»å– system.json å¤±è´¥: %s", e)


def get_sample_count(unprocessed_count):
    while True:
        try:
            sample_count = int(input(f"è¯·è¾“å…¥è¦æ·»åŠ å¤„ç†çš„æ–‡ç« æ•°é‡ï¼ˆä¸è¶…è¿‡æœªå¤„ç†æ–‡ç« æ•°é‡ {unprocessed_count}ï¼‰ï¼š").strip())
            if 1 <= sample_count <= unprocessed_count:
                return sample_count
            else:
                print(f"è¯·è¾“å…¥ä¸€ä¸ªä»‹äº 1 åˆ° {unprocessed_count} çš„æ•´æ•°ã€‚")
        except ValueError:
            print("è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥ä¸€ä¸ªæ•´æ•°ã€‚")


def get_top_n_threshold():
    try:
        top_n = int(input("è¯·è¾“å…¥ä¿ç•™çš„åŒ¹é…æ•°é‡ï¼ˆé»˜è®¤10ï¼‰: ").strip() or 10)
    except:
        top_n = 10
    try:
        threshold = float(input("è¯·è¾“å…¥ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆé»˜è®¤0.6ï¼‰: ").strip() or 0.6)
    except:
        threshold = 0.6
    return top_n, threshold


def main():
    parser = argparse.ArgumentParser(description="æ–‡ç« æ£€ç´¢ä¸åµŒå…¥å¤„ç†")
    parser.add_argument("-q", "--question", type=str, help="è¾“å…¥å•ä¸ªæ£€ç´¢ query")
    parser.add_argument("--debug", action="store_true", help="æ˜¯å¦æ˜¾ç¤º DEBUG ä¿¡æ¯")
    args = parser.parse_args()

    setup_logging(args.debug)

    current_index = load_or_build_index()
    processed_count = update_and_log_index(current_index)

    if args.question:
        query = args.question
        top_n, threshold = get_top_n_threshold()
        handle_query(query, top_n, threshold, processed_count)
    else:
        print(f"{cache_dir}")
        print("è¯·é€‰æ‹©åç»­æ“ä½œ:")
        print("1. æ·»åŠ æ–°çš„åˆ‡ç‰‡å†…å®¹ï¼ˆå¤„ç†æ–°æ–‡ç« ï¼‰")
        print("2. ç›´æ¥è¿›è¡ŒæŸ¥è¯¢")
        print("3. ç”Ÿæˆè‡ªæˆ‘æ—ç™½ï¼ˆæ–°ä¸šåŠ¡ï¼‰")
        choice = input("è¯·è¾“å…¥ 1, 2 æˆ– 3ï¼š").strip()
        if choice == "1":
            add_new_slices()
        elif choice == "2":
            logging.info("ç”¨æˆ·é€‰æ‹©ç›´æ¥è¿›è¡ŒæŸ¥è¯¢")
            sub_choice = input("è¯·é€‰æ‹©æŸ¥è¯¢æ¨¡å¼:\n1. è‡ªå®šä¹‰ query\n2. ä½¿ç”¨ system.json ä¸­é¢„è®¾çš„ query\nè¯·è¾“å…¥ 1 æˆ– 2ï¼š").strip()
            top_n, threshold = get_top_n_threshold()
            if sub_choice == "1":
                query = input("è¯·è¾“å…¥æ£€ç´¢ query: ").strip()
                if query:
                    handle_query(query, top_n, threshold, processed_count)
                else:
                    logging.info("æœªè¿›è¡ŒæŸ¥è¯¢")
            elif sub_choice == "2":
                use_preset_queries(top_n, threshold, processed_count)
            else:
                logging.info("æœªé€‰æ‹©ä»»ä½•æŸ¥è¯¢æ¨¡å¼ï¼Œç¨‹åºé€€å‡º")
        elif choice == "3":
            logging.info("ç”¨æˆ·é€‰æ‹©æ–°ä¸šåŠ¡ï¼šç”Ÿæˆè‡ªæˆ‘æ—ç™½")
            make_inner_monologue_by_kimi()
        else:
            logging.info("æœªé€‰æ‹©ä»»ä½•æ“ä½œï¼Œç¨‹åºé€€å‡º")


if __name__ == "__main__":
    main()
